{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-small\")\n",
    "model = AutoModel.from_pretrained(\"facebook/dinov2-small\")\n",
    "model = model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img = Image.open(\"./grant-headshot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_vects(imgs):\n",
    "    inputs = processor(images=imgs, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(\"mps\") for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    # vects = outputs.last_hidden_state.mean(axis=1).detach().cpu().numpy()\n",
    "    vects = outputs.last_hidden_state[:, 1:, :].detach().cpu().numpy()\n",
    "    # normalize\n",
    "    # vects /= np.linalg.norm(vects, axis=1, keepdims=True)\n",
    "    return vects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:3967: UserWarning: The operator 'aten::upsample_bicubic2d.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  return torch._C._nn.upsample_bicubic2d(input, output_size, align_corners, scale_factors)\n"
     ]
    }
   ],
   "source": [
    "target_vect = get_img_vects(target_img)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelArtDQNEnv(gym.Env):\n",
    "    \"\"\"A custom environment for evolving pixel art using DQN.\"\"\"\n",
    "    def __init__(self, target_image, img_size=32):\n",
    "        super(PixelArtDQNEnv, self).__init__()\n",
    "        self.target_image = target_image\n",
    "        self.target_vect = get_img_vects(target_image)[0]\n",
    "        self.img_size = img_size\n",
    "        # Define the action and observation space\n",
    "        self.action_space = spaces.Discrete(img_size * img_size * 2)  # *2 for toggle actions\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(img_size * img_size,), dtype=np.uint8)\n",
    "        self.state = np.random.randint(2, size=(img_size * img_size), dtype=np.uint8)\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # Determine pixel and state from action\n",
    "        pixel_index, state = divmod(action, 2)\n",
    "        self.state[pixel_index] = state\n",
    "        \n",
    "        # Convert state to image for evaluation\n",
    "        img = Image.fromarray(self.state.reshape(self.img_size, self.img_size) * 255).convert(\"RGB\")\n",
    "        # Implement your similarity calculation here\n",
    "        reward = self.calculate_err(img)\n",
    "        \n",
    "        observation = self.state\n",
    "        info = {}\n",
    "        terminated = False  # e.g., if some termination condition is met\n",
    "        truncated = False  # e.g., if the episode reaches a time limit\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "\n",
    "    def reset(self, **kwargs):  # Updated to accept arbitrary keyword arguments\n",
    "        self.state = np.random.randint(2, size=(self.img_size * self.img_size), dtype=np.uint8)\n",
    "        reset_info = {}  \n",
    "        return self.state, reset_info  # Return both state and reset_info as a tuple\n",
    "\n",
    "    \n",
    "    def calculate_err(self, img):\n",
    "        # Placeholder for similarity calculation logic\n",
    "        # return np.random.random()  # Replace with actual implementation\n",
    "        # img = Image.fromarray(img.reshape(self.img_size, self.img_size) * 255).convert(\"RGB\")\n",
    "        # Calculate similarity to target image (placeholder function)\n",
    "        img_vect = get_img_vects(img)[0]\n",
    "        similarity = ((img_vect - self.target_vect) ** 2).mean()\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x2f74eb4d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize your target image and environment\n",
    "# target_image = np.random.rand(32, 32, 3)  # Replace with the actual target image\n",
    "env = PixelArtDQNEnv(target_image=target_img, img_size=32)\n",
    "\n",
    "rl_model = DQN(\"MlpPolicy\", env, verbose=3, learning_rate=1e-4, buffer_size=10000, learning_starts=1000, batch_size=32, tau=1.0, gamma=0.99, train_freq=4, gradient_steps=1, optimize_memory_usage=False, target_update_interval=500, exploration_fraction=0.1, exploration_initial_eps=1.0, exploration_final_eps=0.05, max_grad_norm=10)\n",
    "rl_model.learn(total_timesteps=20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
