{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 0.5416491627693176\n",
      "Step 100, Loss: 0.17150121927261353\n",
      "Step 200, Loss: 0.11609020084142685\n"
     ]
    }
   ],
   "source": [
    "# Load your target image and preprocess it\n",
    "target_img_path = './grant-headshot.png'  # Update this path\n",
    "target_img = Image.open(target_img_path).convert('RGB')\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "target_img_tensor = transform(target_img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = resnet50(pretrained=True).eval()\n",
    "\n",
    "# Use the feature extractor part only\n",
    "feature_extractor = torch.nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "# Create an image to optimize\n",
    "optimized_img = torch.randn(target_img_tensor.size(), requires_grad=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam([optimized_img], lr=0.01)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Optimize\n",
    "num_steps = 300  # Adjust the number of steps as needed\n",
    "for step in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    # Ensure the optimized image is within valid pixel range\n",
    "    optimized_img_clamped = torch.clamp(optimized_img, 0, 1)\n",
    "    target_features = feature_extractor(target_img_tensor)\n",
    "    optimized_features = feature_extractor(optimized_img_clamped)\n",
    "    loss = loss_fn(optimized_features, target_features)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "# Convert the optimized image to a PIL image for viewing/saving\n",
    "optimized_img_np = optimized_img_clamped.detach().squeeze().permute(1, 2, 0).numpy()\n",
    "optimized_img_pil = Image.fromarray((optimized_img_np * 255).astype(np.uint8))\n",
    "optimized_img_pil.show()  # Or save with optimized_img_pil.save('optimized_image.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
